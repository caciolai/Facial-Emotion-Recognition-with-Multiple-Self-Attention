{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bovqtMyz8IwK"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATCT_p4u7xmV"
   },
   "source": [
    "## Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d2DrfzBc8BHu"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Mtkmtrm70I0"
   },
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade numpy pandas scikit-learn matplotlib seaborn tqdm datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade torch torchtext torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade fastai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDLRMTya8KHX"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5WBWgYEX8XRp"
   },
   "outputs": [],
   "source": [
    "# DL\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from fastai.layers import SelfAttention\n",
    "\n",
    "# PyTorch training framework\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.metrics.functional import accuracy\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "\n",
    "# Preprocessing\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "# CV utilities\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import deepface\n",
    "from deepface.commons import functions\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Plotting utilities\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# General utilities\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y3hSGPOW8Y0v"
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "# device = \"cpu\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9YfZizl8Lmm"
   },
   "source": [
    "# Data acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JFoeA_or_E4b"
   },
   "outputs": [],
   "source": [
    "# # The Kaggle API client expects this file to be in ~/.kaggle,\n",
    "# # so move it there.\n",
    "# !mkdir -p ~/.kaggle\n",
    "# !cp \"/content/drive/MyDrive/Colab Notebooks/kaggle.json\" ~/.kaggle/\n",
    "\n",
    "# # This permissions change avoids a warning on Kaggle tool startup.\n",
    "# !chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sEUrF2lT_arJ"
   },
   "outputs": [],
   "source": [
    "# # Let's make sure the kaggle.json file is present.\n",
    "# !ls -lha ~/.kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ZUy_DXq_MIk"
   },
   "outputs": [],
   "source": [
    "# # List available datasets.\n",
    "# !kaggle datasets list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XT9F-sN2_r_X"
   },
   "outputs": [],
   "source": [
    "# # Copy the stackoverflow data set locally.\n",
    "# !kaggle datasets download -d deadskull7/fer2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4VF-I-YO_0gP"
   },
   "outputs": [],
   "source": [
    "# !unzip fer2013.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YU8-9Bo4ABoh"
   },
   "outputs": [],
   "source": [
    "# fer = pd.read_csv(\"./fer2013.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MhPCByhpBP67"
   },
   "outputs": [],
   "source": [
    "# fer.to_csv(\"/content/drive/MyDrive/Colab Notebooks/FER.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qv7xJT0rBoJW"
   },
   "outputs": [],
   "source": [
    "fer = pd.read_csv(\"./FER.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_yuEzSeZTpGB"
   },
   "outputs": [],
   "source": [
    "# emotion_mapping = {0: 'Anger', 1: 'Disgust', 2: 'Fear', \n",
    "#                    3: 'Happiness', 4: 'Sadness', 5: 'Surprise', 6: 'Neutrality'}\n",
    "\n",
    "# df[\"label\"] = df.apply(lambda row: emotion_mapping[row[\"emotion\"]], axis=1)\n",
    "# df = df.drop([\"emotion\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QDLqKOJ0OSwW"
   },
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fLEgyWqPr2_L"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/microsoft/FERPlus.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RGh5N79Or5if"
   },
   "outputs": [],
   "source": [
    "ferplus = pd.read_csv(\"./FERPlus/fer2013new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8wA2VM7rsANL"
   },
   "outputs": [],
   "source": [
    "def clean_data(fer, ferplus):\n",
    "    # drop usage and emotion in fer\n",
    "    fer = fer.drop([\"Usage\"], axis=1)\n",
    "    # concatenate\n",
    "    df = pd.concat([fer, ferplus], axis=1)\n",
    "    \n",
    "    # keep ferplus labels\n",
    "    df[\"label\"] = df[[\"neutral\", \"happiness\", \"surprise\", \"sadness\", \"anger\", \"disgust\", \"fear\", \"contempt\", \"unknown\", \"NF\"]].idxmax(axis=1)\n",
    "    df = df[[\"pixels\", \"Usage\", \"label\"]]\n",
    "\n",
    "    # get rid of ambiguous faces\n",
    "    df = df.drop(df[df[\"label\"] == \"NF\"].index)\n",
    "\n",
    "    # get rid of unknown/rare emotion\n",
    "    df = df.drop(df[df[\"label\"] == \"contempt\"].index)\n",
    "    df = df.drop(df[df[\"label\"] == \"unknown\"].index)\n",
    "\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FyRr1mJpsGAA"
   },
   "outputs": [],
   "source": [
    "df = clean_data(fer, ferplus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./FER+.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqTtn05F8OgR"
   },
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "noMqnF8GA8yA"
   },
   "outputs": [],
   "source": [
    "n_images = df.shape[0]\n",
    "height = int(np.sqrt(len(df[\"pixels\"][0].split()))) \n",
    "width = int(height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6jeY9URAA1bm"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(10, 10), nrows=3, ncols=3)\n",
    "for i in range(9): \n",
    "    h = i//3\n",
    "    w = i % 3\n",
    "    idx = np.random.randint(0, n_images)\n",
    "    img_arr = np.fromstring(df[\"pixels\"][idx], dtype=int, sep=' ').reshape(height, width)\n",
    "    \n",
    "    axs[h,w].imshow(img_arr, interpolation='none', cmap='gray')\n",
    "    axs[h,w].set_title(df[\"label\"][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0UzGKpnGWT-r"
   },
   "outputs": [],
   "source": [
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QcfgNj2fT4bO"
   },
   "outputs": [],
   "source": [
    "num_classes = df[\"label\"].nunique()\n",
    "num_datapoints = df[\"label\"].count()\n",
    "print(f\"There are {num_classes} different emotion classes across {num_datapoints} datapoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V2hnEhOAL8mT"
   },
   "outputs": [],
   "source": [
    "with sns.axes_style('darkgrid'):\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    sns.countplot(data = df, x = 'label', order = df['label'].value_counts().index,ax=ax)\n",
    "    \n",
    "    for p in ax.patches:\n",
    "        x=p.get_bbox().get_points()[:,0]\n",
    "        y=p.get_bbox().get_points()[1,1]\n",
    "        ax.annotate('{:d}'.format(p.get_height()), (x.mean(), y), ha='center', va='bottom')\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t42tIhFo8Psi"
   },
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G-9kHzJZUTNo"
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(df[\"label\"].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xCLybROlC3Bw"
   },
   "outputs": [],
   "source": [
    "def prepare_data(data, label_encoder):\n",
    "    width = 48\n",
    "    height = 48\n",
    "    X = np.zeros((len(data), height, width), dtype=np.uint8)\n",
    "\n",
    "    y = le.transform(data[\"label\"].to_numpy())\n",
    "    y = torch.Tensor(y).to(torch.long)\n",
    "\n",
    "    for i, row in enumerate(data.index):\n",
    "        pixels = np.fromstring(data['pixels'][row], dtype=int, sep=' ')\n",
    "        image = np.asarray(pixels).reshape(48, 48)\n",
    "        image = image.astype(np.uint8)\n",
    "        X[i] = image\n",
    "        # X[i] = np.expand_dims(image, -1)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n6kftsdxDYtu"
   },
   "outputs": [],
   "source": [
    "X_train, y_train    = prepare_data(df[df['Usage'] == 'Training'], le)\n",
    "X_val, y_val        = prepare_data(df[df['Usage'] == 'PrivateTest'], le)\n",
    "X_test, y_test      = prepare_data(df[df['Usage'] == 'PublicTest'], le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aOvZDUH6Z2-E"
   },
   "outputs": [],
   "source": [
    "print(X_train.shape, X_train.dtype)\n",
    "print(y_train.shape, y_train.dtype)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(X_val.shape, X_val.dtype)\n",
    "print(y_val.shape, y_val.dtype)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(X_test.shape, X_test.dtype)\n",
    "print(y_test.shape, y_test.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVC_HiDVEXtG"
   },
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpRv7G_VY59Q"
   },
   "source": [
    "### Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AYdm9j12ndJY"
   },
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(0.2*np.pi),\n",
    "    # transforms.RandomPerspective(distortion_scale=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.5,),(0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pt7M5J77o_ZP"
   },
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.5,),(0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WzGKDMszY7O5"
   },
   "source": [
    "### N-crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wCcqFAz4Y_Xx"
   },
   "outputs": [],
   "source": [
    "#  mu, st = 0, 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RbAKClVuY8cM"
   },
   "outputs": [],
   "source": [
    "# train_transform = transforms.Compose([\n",
    "#     transforms.ToPILImage(),\n",
    "#     transforms.RandomResizedCrop(48, scale=(0.8, 1.2)),\n",
    "#     transforms.RandomApply([transforms.RandomAffine(0, translate=(0.2, 0.2))], p=0.5),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomApply([transforms.RandomRotation(10)], p=0.5),\n",
    "#     transforms.TenCrop(40),\n",
    "#     transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
    "#     transforms.Lambda(lambda tensors: torch.stack([transforms.Normalize(mean=(mu,), std=(st,))(t) for t in tensors])),\n",
    "#     transforms.Lambda(lambda tensors: torch.stack([transforms.RandomErasing(p=0.5)(t) for t in tensors])),\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2vrqLpEqZF-D"
   },
   "outputs": [],
   "source": [
    "# test_transform = transforms.Compose([\n",
    "#     transforms.ToPILImage(),\n",
    "#     transforms.TenCrop(40),\n",
    "#     transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
    "#     transforms.Lambda(lambda tensors: torch.stack([transforms.Normalize(mean=(mu,), std=(st,))(t) for t in tensors])),\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VgZBEzPMEVGq"
   },
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MnNgkKICCVT9"
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None, augment=False):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        sample = (img, label)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l3Yoo04cFTWb"
   },
   "outputs": [],
   "source": [
    "train_set   = MyDataset(X_train, y_train, train_transform)\n",
    "val_set     = MyDataset(X_val, y_val, test_transform)\n",
    "test_set    = MyDataset(X_test, y_test, test_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ystVEryEbQi"
   },
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TKu1jsY2FjdB"
   },
   "outputs": [],
   "source": [
    "# batch_size = 64\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WgEEV_4uEext"
   },
   "outputs": [],
   "source": [
    "train_loader      = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader      = DataLoader(val_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader     = DataLoader(test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPH4UNEi4p3Y"
   },
   "outputs": [],
   "source": [
    "inputs, labels = next(iter(train_loader))\n",
    "print(inputs.shape, labels.shape)\n",
    "\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "print(np.asarray((le.classes_[unique], counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HRZQB4FkPITg"
   },
   "outputs": [],
   "source": [
    "for batch in test_loader:\n",
    "    inputs, labels = batch\n",
    "    print(inputs.shape)\n",
    "    print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7P0dmFO0qcMg"
   },
   "outputs": [],
   "source": [
    "class BalancedSampler(WeightedRandomSampler):\n",
    "    def __init__(self, dataset):\n",
    "        y = dataset.labels\n",
    "        \n",
    "        class_sample_count = np.array([len(np.where(y==t)[0]) for t in np.unique(y)])\n",
    "        weight = 1. / class_sample_count\n",
    "        samples_weight = np.array([weight[t] for t in y])\n",
    "\n",
    "        samples_weight = torch.from_numpy(samples_weight)\n",
    "        samples_weight = samples_weight.to(torch.double)\n",
    "        \n",
    "        # sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "        super().__init__(samples_weight, len(samples_weight))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qi-2sv7Uphir"
   },
   "outputs": [],
   "source": [
    "train_loader    = DataLoader(train_set, batch_size=batch_size, sampler=BalancedSampler(train_set))\n",
    "val_loader      = DataLoader(val_set, batch_size=batch_size, sampler=BalancedSampler(val_set))\n",
    "test_loader     = DataLoader(test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VGN77VkspmHO"
   },
   "outputs": [],
   "source": [
    "inputs, labels = next(iter(train_loader))\n",
    "print(inputs.shape, labels.shape)\n",
    "\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "print(np.asarray((le.classes_[unique], counts)).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FKSgAK0Rbqy0"
   },
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NiEb3FXvpjJn"
   },
   "outputs": [],
   "source": [
    "sample = next(iter(train_loader))\n",
    "X, y = sample\n",
    "Xi = X[0,...]\n",
    "yi = y[0]\n",
    "\n",
    "img = torch.squeeze(Xi)\n",
    "img = img.numpy()\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.title(le.classes_[yi])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wBOWOUg1pxQ0"
   },
   "outputs": [],
   "source": [
    "sample = next(iter(val_loader))\n",
    "X, y = sample\n",
    "Xi = X[0,...]\n",
    "yi = y[0]\n",
    "\n",
    "img = torch.squeeze(Xi)\n",
    "img = img.numpy()\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.title(le.classes_[yi])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_oQuUZt8RfK"
   },
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ei-I3yGcj63g"
   },
   "outputs": [],
   "source": [
    "class MyLightningModule(pl.LightningModule):\n",
    "    def __init__(self, Ncrops=False):\n",
    "        super().__init__()\n",
    "        self.Ncrops = Ncrops\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "\n",
    "        if self.Ncrops:\n",
    "            # fuse crops and batchsize\n",
    "            bs, ncrops, c, h, w = inputs.shape\n",
    "            inputs = inputs.view(-1, c, h, w)\n",
    "            # repeat labels ncrops times\n",
    "            labels = torch.repeat_interleave(labels, repeats=ncrops, dim=0)\n",
    "\n",
    "        logits = self(inputs)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "\n",
    "        if self.Ncrops:\n",
    "            # fuse crops and batchsize\n",
    "            bs, ncrops, c, h, w = inputs.shape\n",
    "            inputs = inputs.view(-1, c, h, w)\n",
    "            # forward\n",
    "            logits = self(inputs)\n",
    "            # combine results across the crops\n",
    "            logits = logits.view(bs, ncrops, -1)\n",
    "            logits = torch.sum(logits, dim=1) / ncrops\n",
    "        else:\n",
    "            logits = self(inputs)\n",
    "\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        probs = F.log_softmax(logits, dim=1)\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "        acc = accuracy(preds, labels)        \n",
    "        \n",
    "        metrics = {'val_acc': acc, 'val_loss': loss}\n",
    "        self.log_dict(metrics)\n",
    "        return metrics\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "\n",
    "        if self.Ncrops:\n",
    "            # fuse crops and batchsize\n",
    "            bs, ncrops, c, h, w = inputs.shape\n",
    "            inputs = inputs.view(-1, c, h, w)\n",
    "            # forward\n",
    "            logits = self(inputs)\n",
    "            # combine results across the crops\n",
    "            logits = logits.view(bs, ncrops, -1)\n",
    "            logits = torch.sum(logits, dim=1) / ncrops\n",
    "        else:\n",
    "            logits = self(inputs)\n",
    "\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        probs = F.log_softmax(logits, dim=1)\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "        acc = accuracy(preds, labels)     \n",
    "\n",
    "        metrics = {'test_acc': acc, 'test_loss': loss}\n",
    "        self.log_dict(metrics)\n",
    "        return metrics\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx):\n",
    "        inputs, _ = batch\n",
    "        \n",
    "        logits = self(inputs)\n",
    "        probs = F.log_softmax(logits, dim=1)\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6gYT0WWcGv6"
   },
   "source": [
    "## SimpleCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gl3uDrx9bP0b"
   },
   "outputs": [],
   "source": [
    "class SimpleCNN(MyLightningModule):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1a = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=3)\n",
    "        self.conv1b = nn.Conv2d(10, out_channels=10, kernel_size=3)\n",
    "\n",
    "        self.conv2a = nn.Conv2d(10, 10, 3)\n",
    "        self.conv2b = nn.Conv2d(10, 10, 3)\n",
    "\n",
    "        self.lin1 = nn.Linear(10 * 9 * 9, 50)\n",
    "        self.lin2 = nn.Linear(50, num_classes)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.drop = nn.Dropout()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (1, 48, 48) -> (10, 46, 46)\n",
    "        x = F.relu(self.conv1a(x))\n",
    "        # (10, 46, 46) -> (10, 44, 44)\n",
    "        x = F.relu(self.conv1b(x))\n",
    "        # (10, 44, 44) -> (10, 22, 22)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # (10, 22, 22) -> (10, 20, 20)\n",
    "        x = F.relu(self.conv2a(x))\n",
    "        # (10, 20, 20) -> (10, 18, 18)\n",
    "        x = F.relu(self.conv2b(x))\n",
    "        # (10, 18, 18) -> (10, 9, 9)\n",
    "        x = self.pool(x)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        # (10, 9, 9) -> (10 * 9 * 9,)\n",
    "        x = x.view(-1, 10 * 9 * 9)\n",
    "        # (10 * 9 * 9,) -> (50,)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        # (50,) -> (num_classes,)\n",
    "        x = self.lin2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        optimizer = torch.optim.SGD(\n",
    "            self.parameters(), lr=0.01, momentum=0.9, nesterov=True, \n",
    "            weight_decay=0.0001)\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='max', factor=0.5, patience=2, verbose=False)\n",
    "        \n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': scheduler,\n",
    "            'monitor': 'val_acc'\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Luw_IoycKp-"
   },
   "source": [
    "## Deep-Emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPrkC_DP8JSd"
   },
   "outputs": [],
   "source": [
    "class DeepEmotion(MyLightningModule):\n",
    "    def __init__(self, num_classes):\n",
    "        '''\n",
    "        https://github.com/omarsayed7/Deep-Emotion/blob/master/deep_emotion.py\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1,10,3)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(10,10,3)\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(10,10,3)\n",
    "        self.conv4 = nn.Conv2d(10,10,3)\n",
    "        self.pool4 = nn.MaxPool2d(2,2)\n",
    "\n",
    "        self.norm = nn.BatchNorm2d(10)\n",
    "\n",
    "        self.fc1 = nn.Linear(810,50)\n",
    "        self.fc2 = nn.Linear(50,num_classes)\n",
    "\n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=7),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 10, kernel_size=5),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(640, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 3 * 2)\n",
    "        )\n",
    "        self.fc_loc[2].weight.data.zero_()\n",
    "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "\n",
    "    def stn(self, x):\n",
    "        xs = self.localization(x)\n",
    "        xs = xs.view(-1, 640)\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "\n",
    "        grid = F.affine_grid(theta, x.size())\n",
    "        x = F.grid_sample(x, grid)\n",
    "        return x, grid\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.stn(x)\n",
    "\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.pool2(x))\n",
    "\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.norm(self.conv4(x))\n",
    "        x = F.relu(self.pool4(x))\n",
    "\n",
    "        # out = F.dropout(out)\n",
    "        x = x.view(-1, 810)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        optimizer = torch.optim.SGD(\n",
    "            self.parameters(), lr=0.01, momentum=0.9, nesterov=True, \n",
    "            weight_decay=0.0001)\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='max', factor=0.5, patience=2, verbose=False)\n",
    "        \n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': scheduler,\n",
    "            'monitor': 'val_acc'\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zmT6RNxkBHfy"
   },
   "source": [
    "## MultipleSelfAttention module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yfx4jsx__4M2"
   },
   "outputs": [],
   "source": [
    "class MultipleSelfAttention(nn.Module):    \n",
    "    def __init__(self, shape, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ch, self.h, self.w = shape\n",
    "        self.n = num_heads\n",
    "\n",
    "        n, ch, w, h = self.n, self.ch, self.w, self.h\n",
    "        # hybrid attention\n",
    "        self.attention = SelfAttention(self.n*self.ch)\n",
    "        \n",
    "        # weight learning\n",
    "        self.conv = nn.Conv2d(in_channels=ch, out_channels=ch, kernel_size=1, stride=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        h, w = h//2, w//2\n",
    "        self.fc = nn.Linear(ch*h*w, num_heads)\n",
    "\n",
    "    def forward(self, x):\n",
    "        n, ch, w, h = self.n, self.ch, self.w, self.h\n",
    "        \n",
    "        # hybrid attention\n",
    "        xh = x.repeat(1, n, 1, 1)\n",
    "        xh = self.attention(xh)\n",
    "        xh = xh.view(-1, n, ch, h, w) # (bs, N, ch, h, w)\n",
    "        \n",
    "        # weight learning\n",
    "        # (bs, ch, h, w) -> (bs, ch, h, w)\n",
    "        xs = self.conv(x) \n",
    "        # (bs, ch, h, w) -> (bs, ch, h/2, w/2)\n",
    "        xs = self.pool(x) \n",
    "        # (bs, ch, h/2, w/2) -> (bs, ch*h/2*w/2)\n",
    "        h, w = h//2, w//2\n",
    "        xs = xs.view(-1, ch*h*w)\n",
    "        # (bs, ch*h/2*w/2) -> (bs, n)\n",
    "        xs = F.sigmoid(self.fc(xs))\n",
    "        xs = F.normalize(xs, p=1, dim=1) # obtain probabilities\n",
    "\n",
    "        # weighted sum\n",
    "        x = torch.sum(torch.mul(xh, xs[:,:,None,None,None]), dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfeF6fbBcNeC"
   },
   "source": [
    "## Deep-Emotion-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vrvnZrN1BCOD"
   },
   "outputs": [],
   "source": [
    "class DeepEmotionAttention(MyLightningModule):\n",
    "    def __init__(self, num_classes):\n",
    "        '''\n",
    "        https://github.com/omarsayed7/Deep-Emotion/blob/master/deep_emotion.py\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1,10,3)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(10,10,3)\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(10,10,3)\n",
    "        self.conv4 = nn.Conv2d(10,10,3)\n",
    "        self.pool4 = nn.MaxPool2d(2,2)\n",
    "\n",
    "        self.norm = nn.BatchNorm2d(10)\n",
    "\n",
    "        self.fc1 = nn.Linear(810,50)\n",
    "        self.fc2 = nn.Linear(50,num_classes)\n",
    "\n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=7),\n",
    "            MultipleSelfAttention((8, 42, 42), 8),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 10, kernel_size=5),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(640, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 3 * 2)\n",
    "        )\n",
    "        self.fc_loc[2].weight.data.zero_()\n",
    "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "\n",
    "    def stn(self, x):\n",
    "        xs = self.localization(x)\n",
    "        xs = xs.view(-1, 640)\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "\n",
    "        grid = F.affine_grid(theta, x.size())\n",
    "        x = F.grid_sample(x, grid)\n",
    "        return x, grid\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.stn(x)\n",
    "\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.pool2(x))\n",
    "\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.norm(self.conv4(x))\n",
    "        x = F.relu(self.pool4(x))\n",
    "\n",
    "        # out = F.dropout(out)\n",
    "        x = x.view(-1, 810)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        optimizer = torch.optim.SGD(\n",
    "            self.parameters(), lr=0.01, momentum=0.9, nesterov=True, \n",
    "            weight_decay=0.0001)\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='max', factor=0.5, patience=2, verbose=False)\n",
    "        \n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': scheduler,\n",
    "            'monitor': 'val_acc'\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7yHrPaYA-Fe"
   },
   "source": [
    "## VGGAttentionFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dBOfIJ042JLN"
   },
   "outputs": [],
   "source": [
    "class SpatialTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=7),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 10, kernel_size=5),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(640, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 3 * 2)\n",
    "        )\n",
    "        self.fc_loc[2].weight.data.zero_()\n",
    "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "\n",
    "    def forward(self, x):\n",
    "        xs = self.localization(x)\n",
    "        xs = xs.view(-1, 640)\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "\n",
    "        grid = F.affine_grid(theta, x.size())\n",
    "        x = F.grid_sample(x, grid)\n",
    "        return x, grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mdp5HY5A-Ebz"
   },
   "outputs": [],
   "source": [
    "class VGGFaceAttention(MyLightningModule):\n",
    "    def __init__(self, num_classes, Ncrops=False):\n",
    "        super().__init__(Ncrops)\n",
    "\n",
    "        self.conv1a = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding='same')\n",
    "        self.conv1b = nn.Conv2d(64, out_channels=64, kernel_size=3, padding='same')\n",
    "\n",
    "        self.msa = MultipleSelfAttention((64, 24, 24), 8) # combine N attention heads\n",
    "\n",
    "        self.conv2a = nn.Conv2d(64, 128, 3, padding='same')\n",
    "        self.conv2b = nn.Conv2d(128, 128, 3, padding='same')\n",
    "\n",
    "        self.conv3a = nn.Conv2d(128, 256, 3, padding='same')\n",
    "        self.conv3b = nn.Conv2d(256, 256, 3, padding='same')\n",
    "\n",
    "        self.conv4a = nn.Conv2d(256, 512, 3, padding='same')\n",
    "        self.conv4b = nn.Conv2d(512, 512, 3, padding='same')\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.bn1a = nn.BatchNorm2d(64)\n",
    "        self.bn1b = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.bn2a = nn.BatchNorm2d(128)\n",
    "        self.bn2b = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.bn3a = nn.BatchNorm2d(256)\n",
    "        self.bn3b = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.bn4a = nn.BatchNorm2d(512)\n",
    "        self.bn4b = nn.BatchNorm2d(512)\n",
    "\n",
    "        self.lin1 = nn.Linear(512 * 3 * 3, 4096)\n",
    "        self.lin2 = nn.Linear(4096, 4096)\n",
    "        self.lin3 = nn.Linear(4096, num_classes)\n",
    "\n",
    "        self.drop = nn.Dropout()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (1, 48, 48) -> (64, 24, 24)\n",
    "        x = F.relu(self.bn1a(self.conv1a(x)))\n",
    "        x = F.relu(self.bn1b(self.conv1b(x)))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # (64, 24, 24) -> (64, 24, 24)\n",
    "        x = self.msa(x) # apply multiple self attention\n",
    "\n",
    "        # (64, 24, 24) -> (128, 12, 12)\n",
    "        x = F.relu(self.bn2a(self.conv2a(x)))\n",
    "        x = F.relu(self.bn2b(self.conv2b(x)))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # (128, 12, 12) -> (256, 6, 6)\n",
    "        x = F.relu(self.bn3a(self.conv3a(x)))\n",
    "        x = F.relu(self.bn3b(self.conv3b(x)))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # (256, 6, 6) -> (512, 3, 3)\n",
    "        x = F.relu(self.bn4a(self.conv4a(x)))\n",
    "        x = F.relu(self.bn4b(self.conv4b(x)))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = x.view(-1, 512 * 3 * 3)\n",
    "        x = F.relu(self.drop(self.lin1(x)))\n",
    "        x = F.relu(self.drop(self.lin2(x)))\n",
    "        x = self.lin3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        optimizer = torch.optim.SGD(\n",
    "            self.parameters(), lr=0.01, momentum=0.9, nesterov=True, \n",
    "            weight_decay=0.0001)\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='max', factor=0.5, patience=2, verbose=False)\n",
    "        \n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': scheduler,\n",
    "            'monitor': 'val_acc'\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WwjFdCPvnGr"
   },
   "source": [
    "## VGGFaceAttentionSTN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.localization = nn.Sequential(\n",
    "            # (64, 24, 24) -> (64, 20, 20)\n",
    "            nn.Conv2d(64, 8*64, kernel_size=5),\n",
    "            # (64, 20, 20) -> (64, 10, 10)\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "            # (64, 10, 10) -> (64, 6, 6)\n",
    "            nn.Conv2d(8*64, 10*64, kernel_size=5),\n",
    "            # (64, 6, 6) -> (64, 3, 3)\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(64*10*3*3, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 3 * 2)\n",
    "        )\n",
    "        self.fc_loc[2].weight.data.zero_()\n",
    "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "\n",
    "    def forward(self, x):\n",
    "        xs = self.localization(x)\n",
    "        xs = xs.view(-1, 64*10*3*3)\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "\n",
    "        grid = F.affine_grid(theta, x.size())\n",
    "        x = F.grid_sample(x, grid)\n",
    "        return x, grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "44ZUFxpR2Xmw"
   },
   "outputs": [],
   "source": [
    "class VGGFaceAttentionSTN(MyLightningModule):\n",
    "    def __init__(self, num_classes, Ncrops=False):\n",
    "        super().__init__(Ncrops)\n",
    "\n",
    "        \n",
    "        self.conv1a = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding='same')\n",
    "        self.conv1b = nn.Conv2d(64, out_channels=64, kernel_size=3, padding='same')\n",
    "        \n",
    "        self.stn = SpatialTransformer()\n",
    "        self.msa = MultipleSelfAttention((64, 24, 24), 8) # combine N attention heads\n",
    "\n",
    "        self.conv2a = nn.Conv2d(64, 128, 3, padding='same')\n",
    "        self.conv2b = nn.Conv2d(128, 128, 3, padding='same')\n",
    "\n",
    "        self.conv3a = nn.Conv2d(128, 256, 3, padding='same')\n",
    "        self.conv3b = nn.Conv2d(256, 256, 3, padding='same')\n",
    "\n",
    "        self.conv4a = nn.Conv2d(256, 512, 3, padding='same')\n",
    "        self.conv4b = nn.Conv2d(512, 512, 3, padding='same')\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.bn1a = nn.BatchNorm2d(64)\n",
    "        self.bn1b = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.bn2a = nn.BatchNorm2d(128)\n",
    "        self.bn2b = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.bn3a = nn.BatchNorm2d(256)\n",
    "        self.bn3b = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.bn4a = nn.BatchNorm2d(512)\n",
    "        self.bn4b = nn.BatchNorm2d(512)\n",
    "\n",
    "        self.lin1 = nn.Linear(512 * 3 * 3, 4096)\n",
    "        self.lin2 = nn.Linear(4096, 4096)\n",
    "        self.lin3 = nn.Linear(4096, num_classes)\n",
    "\n",
    "        self.drop = nn.Dropout()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (1, 48, 48) -> (64, 24, 24)\n",
    "        x = F.relu(self.bn1a(self.conv1a(x)))\n",
    "        x = F.relu(self.bn1b(self.conv1b(x)))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # (64, 24, 24) -> (64, 24, 24)\n",
    "        x, _ = self.stn(x)\n",
    "        x = self.msa(x) # apply multiple self attention\n",
    "\n",
    "        # (64, 24, 24) -> (128, 12, 12)\n",
    "        x = F.relu(self.bn2a(self.conv2a(x)))\n",
    "        x = F.relu(self.bn2b(self.conv2b(x)))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # (128, 12, 12) -> (256, 6, 6)\n",
    "        x = F.relu(self.bn3a(self.conv3a(x)))\n",
    "        x = F.relu(self.bn3b(self.conv3b(x)))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # (256, 6, 6) -> (512, 3, 3)\n",
    "        x = F.relu(self.bn4a(self.conv4a(x)))\n",
    "        x = F.relu(self.bn4b(self.conv4b(x)))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = x.view(-1, 512 * 3 * 3)\n",
    "        x = F.relu(self.drop(self.lin1(x)))\n",
    "        x = F.relu(self.drop(self.lin2(x)))\n",
    "        x = self.lin3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        optimizer = torch.optim.SGD(\n",
    "            self.parameters(), lr=0.01, momentum=0.9, nesterov=True, \n",
    "            weight_decay=0.0001)\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='max', factor=0.5, patience=2, verbose=False)\n",
    "        \n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': scheduler,\n",
    "            'monitor': 'val_acc'\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvbkyOFw8Svs"
   },
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSGQ-oHxCSpF"
   },
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ITyOv7V7k7jt"
   },
   "outputs": [],
   "source": [
    "model = SimpleCNN(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ho6u1E5u9IQ"
   },
   "outputs": [],
   "source": [
    "# model = DeepEmotion(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "My12-nVMCHBG"
   },
   "outputs": [],
   "source": [
    "# model = DeepEmotionAttention(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "amiWCYfWlqmn"
   },
   "outputs": [],
   "source": [
    "# model = VGGFaceAttention(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y59ufVlIXRPk"
   },
   "outputs": [],
   "source": [
    "# model = VGGFaceAttentionSTN(num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZeRzF4Koxh2c"
   },
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "54AcGjSblwhd"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "!rm -rf ./logs\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rhh4Oogq0KIq"
   },
   "outputs": [],
   "source": [
    "num_epochs = 300\n",
    "\n",
    "tb_logger = pl_loggers.TensorBoardLogger('./logs/')\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "   monitor='val_acc',\n",
    "   min_delta=0.00,\n",
    "   patience=num_epochs/10,\n",
    "   verbose=False,\n",
    "   mode='max'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1H-ZwoccD0GE"
   },
   "source": [
    "## Training loop (PL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UPZ37wLQxmp0"
   },
   "outputs": [],
   "source": [
    "# trainer = pl.Trainer(max_epochs=num_epochs, gpus=-1, logger=tb_logger, callbacks=[early_stop_callback])\n",
    "trainer = pl.Trainer(max_epochs=num_epochs, gpus=-1, callbacks=[early_stop_callback])\n",
    "# trainer = pl.Trainer(max_epochs=num_epochs, gpus=-1, logger=tb_logger)\n",
    "# trainer = pl.Trainer(max_epochs=num_epochs, gpus=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pm9Mu12_MqTo"
   },
   "outputs": [],
   "source": [
    "trainer.fit(model, train_dataloader=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3QOGwLNb4XUG"
   },
   "source": [
    "## Training loop (Custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "elR8rQboRSEV"
   },
   "outputs": [],
   "source": [
    "# num_epochs = 100\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=0.0001)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# model = model.to(device)\n",
    "# model.train()\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     try:\n",
    "#         with tqdm(train_loader, unit=\"batch\", leave=False) as tepoch:\n",
    "#             for batch in tepoch:\n",
    "                \n",
    "#                 tepoch.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "                \n",
    "#                 inputs, labels = batch\n",
    "#                 inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "#                 optimizer.zero_grad()\n",
    "                \n",
    "#                 logits = model(inputs)\n",
    "#                 loss = criterion(logits, labels)\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "            \n",
    "#                 tepoch.set_postfix(loss=loss.item())\n",
    "#     except KeyboardInterrupt:\n",
    "#         break\n",
    "\n",
    "# print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPeD0FjvQNHe"
   },
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SQe6WOzSQiPy"
   },
   "outputs": [],
   "source": [
    "state_dict_dir = \"/content/drive/MyDrive/Colab Notebooks/weights/\"\n",
    "\n",
    "state_dict_name = f\"{model.__class__.__name__}_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.pt\"\n",
    "\n",
    "state_dict_path = os.path.join(state_dict_dir, state_dict_name)\n",
    "print(state_dict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aeJWm-JsQOyx"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), state_dict_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jy3g0HB8T_s"
   },
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xjp7ek4fUAua"
   },
   "outputs": [],
   "source": [
    "state_dict_dir = \"./\"\n",
    "for f in os.listdir(state_dict_dir):\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TG67si_-dVOh"
   },
   "outputs": [],
   "source": [
    "# state_dict_name = \"DeepEmotion_2021-07-10_15-10-43.pt\"\n",
    "# state_dict_name = \"DeepEmotionAttention_2021-07-11_08-25-46.pt\"\n",
    "# state_dict_name = \"VGGFaceAttention_2021-07-09_10-25-38.pt\"\n",
    "state_dict_name = \"VGGFaceAttentionSTN_2021-07-11_15-16-48.pt\"\n",
    "\n",
    "state_dict_path = os.path.join(state_dict_dir, state_dict_name)\n",
    "print(state_dict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TmkWJ9E68LJP"
   },
   "outputs": [],
   "source": [
    "# model = SimpleCNN(num_classes)\n",
    "# model = DeepEmotion(num_classes)\n",
    "# model = DeepEmotionAttention(num_classes)\n",
    "# model = VGGFaceAttention(num_classes)\n",
    "model = VGGFaceAttentionSTN(num_classes)\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load(state_dict_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3B2cb5s5UFmS"
   },
   "source": [
    "## Test set accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KOx7MnAmeGfe"
   },
   "outputs": [],
   "source": [
    "tester = pl.Trainer(gpus=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L64n3kYReDdK"
   },
   "outputs": [],
   "source": [
    "tester.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23SSqTlsUK12"
   },
   "source": [
    "## Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "umIuAq1QVZAz"
   },
   "outputs": [],
   "source": [
    "y_preds = tester.predict(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Avi1MIimL-E4"
   },
   "outputs": [],
   "source": [
    "y_true = list()\n",
    "y_pred = list()\n",
    "\n",
    "for i,batch in enumerate(test_loader):\n",
    "    inputs, labels = batch\n",
    "\n",
    "    y_true += labels.tolist()\n",
    "    y_pred += y_preds[i].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pS7GaI9qUI6D"
   },
   "outputs": [],
   "source": [
    "target_names = le.classes_\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ikl9sulnUHwU"
   },
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FmXcgR5mUrFe"
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "\n",
    "# df_cm = pd.DataFrame(cm, target_names, target_names)\n",
    "# plt.figure(figsize=(10,7))\n",
    "# sns.set(font_scale=1.4) # for label size\n",
    "# sns.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)\n",
    "\n",
    "disp.plot(ax=ax, cmap=plt.cm.Blues)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# from google.colab import files\n",
    "# filename = \"cm-deepemotion.svg\"\n",
    "# filename = \"cm-deepemotion-attention.svg\"\n",
    "# filename = \"cm-vggface-attention.svg\"\n",
    "filename = \"cm-vggface-attention-stn.svg\"\n",
    "# plt.savefig(filename)\n",
    "# files.download(filename) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GL-8coyMUJWK"
   },
   "source": [
    "## Wrong predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3LWLNfg2S3sE"
   },
   "outputs": [],
   "source": [
    "X_wrong = []\n",
    "y_wrong_true = []\n",
    "y_wrong_pred = []\n",
    "for idx, (y, y_hat) in enumerate(zip(y_true, y_pred)):\n",
    "    if y != y_hat:\n",
    "        X_wrong.append(test_set[idx][0])\n",
    "        y_wrong_true.append(y)\n",
    "        y_wrong_pred.append(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EF8YwZJi11pk"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=3, ncols=3, figsize=(10,10))\n",
    "\n",
    "\n",
    "for i in range(axs.shape[0]):\n",
    "    for j in range(axs.shape[1]):\n",
    "        ax = axs[i,j]\n",
    "        rand_idx = np.random.randint(0, len(X_wrong)-1)\n",
    "        X, y, y_hat = X_wrong[rand_idx], y_wrong_true[rand_idx], y_wrong_pred[rand_idx]\n",
    "\n",
    "        img = torch.squeeze(X)\n",
    "        img = img.numpy()\n",
    "        ax.imshow(img, cmap='gray')\n",
    "        ax.set_title(f\"true: {le.classes_[y]}, pred: {le.classes_[y_hat]}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face detection and alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_align_preprocess_face(img):    \n",
    "    # detect and align\n",
    "    face_img, region = functions.detect_face(\n",
    "        img = img, \n",
    "        detector_backend = 'mtcnn', \n",
    "        grayscale = False, \n",
    "        enforce_detection = False, \n",
    "        align = True)\n",
    "\n",
    "    if face_img.shape[0] == 0 or img.shape[1] == 0: # not detected\n",
    "        face_img = None\n",
    "        \n",
    "    # preprocess\n",
    "    if face_img is not None:\n",
    "        # draw rectangle\n",
    "        x = region[0]; y = region[1]\n",
    "        w = region[2]; h = region[3]\n",
    "        img = cv2.rectangle(img, (x,y), (x+w,y+h), (0,255,0), 4)\n",
    "\n",
    "        face_img = np.squeeze(face_img)\n",
    "        face_img = cv2.cvtColor(face_img, cv2.COLOR_BGR2GRAY)\n",
    "        face_img = cv2.resize(face_img, (48, 48))\n",
    "    \n",
    "    return img, region, face_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotion prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(model, img, transform, device):\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    img = transform(img)\n",
    "    img = torch.unsqueeze(img, dim=0)\n",
    "    img = img.to(device)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        logits = model(img)\n",
    "        probs = F.softmax(logits)\n",
    "        # preds = torch.argmax(probs, dim=-1)\n",
    "        # pred = preds.cpu().item()\n",
    "    \n",
    "    # return label_encoder.classes_[pred]\n",
    "    return torch.squeeze(probs).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_emotion_predictions(img, region, emotion_probabilities, label_encoder):\n",
    "    \"\"\"\n",
    "    https://github.com/serengil/deepface/blob/master/deepface/commons/realtime.py\n",
    "    \"\"\"\n",
    "    mood_items = []\n",
    "    emotion_labels = label_encoder.classes_\n",
    "    sum_of_predictions = emotion_probabilities.sum()\n",
    "    # print(emotion_probabilities.shape)\n",
    "    for i in range(0, len(emotion_labels)):\n",
    "        mood_item = []\n",
    "        emotion_label = emotion_labels[i]\n",
    "        emotion_prediction = 100 * emotion_probabilities[i] / sum_of_predictions\n",
    "        mood_item.append(emotion_label)\n",
    "        mood_item.append(emotion_prediction)\n",
    "        mood_items.append(mood_item)\n",
    "\n",
    "    emotion_df = pd.DataFrame(mood_items, columns = [\"emotion\", \"score\"])\n",
    "    emotion_df = emotion_df.sort_values(by = [\"score\"], ascending=False).reset_index(drop=True)\n",
    "\n",
    "    #background of mood box\n",
    "    resolution_x = img.shape[1]; resolution_y = img.shape[0]\n",
    "    overlay = img.copy()\n",
    "    opacity = 0.4\n",
    "\n",
    "    x = region[0]; y = region[1]\n",
    "    w = region[2]; h = region[3]\n",
    "    # pivot_img_size = 112\n",
    "    pivot_img_size = 150\n",
    "    # text_size = 70\n",
    "    text_size = 100\n",
    "    \n",
    "    if x+w+pivot_img_size < resolution_x:\n",
    "        #right\n",
    "        img = cv2.rectangle(img\n",
    "            #, (x+w,y+20)\n",
    "            , (x+w,y)\n",
    "            , (x+w+pivot_img_size, y+h)\n",
    "            , (64,64,64),cv2.FILLED)\n",
    "\n",
    "        img = cv2.addWeighted(overlay, opacity, img, 1 - opacity, 0, img)\n",
    "\n",
    "    elif x-pivot_img_size > 0:\n",
    "        #left\n",
    "        img = cv2.rectangle(img\n",
    "            #, (x-pivot_img_size,y+20)\n",
    "            , (x-pivot_img_size,y)\n",
    "            , (x, y+h)\n",
    "            , (64,64,64),cv2.FILLED)\n",
    "\n",
    "        img = cv2.addWeighted(overlay, opacity, img, 1 - opacity, 0, img)\n",
    "\n",
    "    for index, instance in emotion_df.iterrows():\n",
    "        emotion_label = \"%s \" % (instance['emotion'])\n",
    "        emotion_score = instance['score']/100\n",
    "\n",
    "        bar_x = 35 #this is the size if an emotion is 100%\n",
    "        bar_x = int(bar_x * emotion_score)\n",
    "\n",
    "        if x+w+pivot_img_size < resolution_x:\n",
    "\n",
    "            text_location_y = y + 20 + (index+1) * 20\n",
    "            text_location_x = x+w\n",
    "\n",
    "            if text_location_y < y + h:\n",
    "                img = cv2.putText(img, emotion_label, (text_location_x, text_location_y), \n",
    "                                  cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "\n",
    "                img = cv2.rectangle(img\n",
    "                    , (x+w+text_size, y + 13 + (index+1) * 20)\n",
    "                    , (x+w+text_size+bar_x, y + 13 + (index+1) * 20 + 5)\n",
    "                    , (255,255,255), cv2.FILLED)\n",
    "\n",
    "        elif x-pivot_img_size > 0:\n",
    "\n",
    "            text_location_y = y + 20 + (index+1) * 20\n",
    "            text_location_x = x-pivot_img_size\n",
    "\n",
    "            if text_location_y <= y+h:\n",
    "                img = cv2.putText(img, emotion_label, (text_location_x, text_location_y), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "\n",
    "                img = cv2.rectangle(img\n",
    "                    , (x-pivot_img_size+text_size, y + 13 + (index+1) * 20)\n",
    "                    , (x-pivot_img_size+text_size+bar_x, y + 13 + (index+1) * 20 + 5)\n",
    "                    , (255,255,255), cv2.FILLED)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(frame):\n",
    "    img, region, face_img = detect_align_preprocess_face(frame)\n",
    "    \n",
    "    if face_img is not None:\n",
    "        \n",
    "        emotion_probabilities = predict_emotion(model, face_img, test_transform, device)\n",
    "        display_emotion_predictions(img, region, emotion_probabilities, label_encoder)\n",
    "        \n",
    "        # sys.stdout.write(prediction + '\\r')\n",
    "    else:\n",
    "        face_img = np.zeros((48,48)).astype(np.uint8)\n",
    "        \n",
    "    # Display the resulting frame\n",
    "    face_img = cv2.resize(face_img, (240, 240), interpolation = cv2.INTER_AREA)\n",
    "    cv2.imshow('frame', img)\n",
    "    cv2.imshow('face', face_img)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Cannot open camera\")\n",
    "        exit()\n",
    "    \n",
    "    while True:\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        # if frame is read correctly ret is True\n",
    "        if not ret:\n",
    "            print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "            break\n",
    "\n",
    "        # Our operations on the frame come here    \n",
    "        process_frame(frame)\n",
    "\n",
    "        if cv2.waitKey(1) == ord('q'): # break by pressing 'q' key\n",
    "            break\n",
    "finally:\n",
    "    # When everything done, release the capture\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Seminars.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
